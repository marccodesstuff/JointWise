{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:06:52.302858Z",
     "start_time": "2025-08-22T07:06:52.300122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from jointwise.preprocessing import DataProcessor, ImprovedSubjectLevelSplitter, MRIAugmentationWithBboxes, BoundingBoxAwareOversampler, MRIDatasetWithBboxes\n",
    "from jointwise.model import BaseModelBuilder, MultiTaskModel, MultiTaskTrainer\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ],
   "id": "192bed5cbccf77b9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:06:52.337425Z",
     "start_time": "2025-08-22T07:06:52.309321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Set deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Environment setup complete.\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "a4c362a8b1d28fb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n",
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3060\n",
      "CUDA memory: 12.0 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "d80e46b8d690b6a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:06:53.505997Z",
     "start_time": "2025-08-22T07:06:52.346454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_processor = DataProcessor('annotations/knee.csv', 'png-output')\n",
    "annotations = data_processor.load_annotations()\n",
    "subject_labels = data_processor.create_target_labels()\n",
    "available_images = data_processor.get_available_images()\n",
    "\n",
    "print(f\"\\nReady to process {len(available_images)} images from {len(subject_labels)} subject entries\")\n",
    "\n",
    "# Let's also check if there are any cases that were previously \"Both\"\n",
    "print(\"\\nChecking for cases with both ACL and Meniscus tears:\")\n",
    "original_files = set()\n",
    "acl_files = set()\n",
    "meniscus_files = set()\n",
    "\n",
    "for file_id, label in subject_labels.items():\n",
    "    if file_id.endswith('_ACL'):\n",
    "        acl_files.add(file_id[:-4])  # Remove _ACL suffix\n",
    "    elif file_id.endswith('_Meniscus'):\n",
    "        meniscus_files.add(file_id[:-9])  # Remove _Meniscus suffix\n",
    "    else:\n",
    "        original_files.add(file_id)\n",
    "\n",
    "both_cases = acl_files.intersection(meniscus_files)\n",
    "print(f\"Found {len(both_cases)} cases that have both ACL and Meniscus tears (now treated as separate entries)\")\n",
    "if len(both_cases) > 0:\n",
    "    print(f\"Examples: {list(both_cases)[:5]}\")  # Show first 5 examples"
   ],
   "id": "f431a8472812d393",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16167 annotations\n",
      "Unique files: 974\n",
      "Label distribution:\n",
      "label\n",
      "Meniscus Tear                                5658\n",
      "Cartilage - Partial Thickness loss/defect    2985\n",
      "Joint Effusion                               1311\n",
      "Bone-Fracture/Contusion/dislocation          1060\n",
      "Bone- Subchondral edema                       986\n",
      "Periarticular cysts                           864\n",
      "Ligament - ACL Low Grade sprain               765\n",
      "Ligament - ACL High Grade Sprain              677\n",
      "Cartilage - Full Thickness loss/defect        615\n",
      "Ligament - MCL Low-Mod Grade Sprain           285\n",
      "Displaced Meniscal Tissue                     232\n",
      "Bone - Lesion                                 183\n",
      "Ligament - PCL Low-Mod grade sprain           142\n",
      "LCL Complex - Low-Mod Grade Sprain            130\n",
      "Soft Tissue Lesion                             90\n",
      "Muscle Strain                                  65\n",
      "Joint Bodies                                   38\n",
      "Patellar Retinaculum - High grade sprain       24\n",
      "Ligament - PCL High Grade                      18\n",
      "LCL Complex- High Grade Sprain                 14\n",
      "artifact                                       13\n",
      "Ligament - MCL High Grade sprain               11\n",
      "Ligament -  High Grade Sprain                   1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Subject-level label distribution:\n",
      "Meniscus_tear    663\n",
      "ACL_tear         254\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Found 6296 available images for ACL/Meniscus Tear\n",
      "\n",
      "Ready to process 6296 images from 917 subject entries\n",
      "\n",
      "Checking for cases with both ACL and Meniscus tears:\n",
      "Found 164 cases that have both ACL and Meniscus tears (now treated as separate entries)\n",
      "Examples: ['file1001721', 'file1002356', 'file1001206', 'file1000971', 'file1000141']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:06:53.556276Z",
     "start_time": "2025-08-22T07:06:53.538392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the improved subject-level splitter with balanced oversampling\n",
    "print(\"Testing data splitting with the new approach...\")\n",
    "\n",
    "# No filtering needed since DataProcessor no longer creates 'Neither' labels\n",
    "final_images = available_images  # Direct use, no filtering step needed\n",
    "print(f\"Images ready for training: {len(final_images)}\")\n",
    "\n",
    "# Show label distribution\n",
    "filtered_labels = [img['label'] for img in final_images]\n",
    "label_dist = pd.Series(filtered_labels).value_counts()\n",
    "print(\"Label distribution:\")\n",
    "print(label_dist)\n",
    "\n",
    "# For stratification, we need to work at subject level\n",
    "# Group images by subject and get one label per subject (handling dual pathology cases)\n",
    "subject_groups = {}\n",
    "for img in final_images:\n",
    "    file_id = img['file_id']\n",
    "    label = img['label']\n",
    "\n",
    "    if file_id not in subject_groups:\n",
    "        subject_groups[file_id] = []\n",
    "    subject_groups[file_id].append(label)\n",
    "\n",
    "# For stratification, use primary pathology (if both, use first one found)\n",
    "subject_stratify_labels = []\n",
    "original_subjects = []\n",
    "\n",
    "for file_id, labels in subject_groups.items():\n",
    "    original_subjects.append(file_id)\n",
    "    # Use the first label for stratification (ACL vs Meniscus)\n",
    "    subject_stratify_labels.append(labels[0])\n",
    "\n",
    "print(f\"Total original subjects: {len(original_subjects)}\")\n",
    "\n",
    "# Show subject stratification distribution\n",
    "stratify_dist = pd.Series(subject_stratify_labels).value_counts()\n",
    "print(\"Subject stratification distribution:\")\n",
    "print(stratify_dist)\n",
    "\n",
    "train_data, temp_data = train_test_split(final_images, test_size=0.3, random_state=42, stratify=[img['label'] for img in final_images])\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=[img['label'] for img in temp_data])\n",
    "\n",
    "# And update the print statements to:\n",
    "print(f\"\\nData split completed (image-level):\")\n",
    "print(f\"Train: {len(train_data)} images\")\n",
    "print(f\"Val: {len(val_data)} images\")\n",
    "print(f\"Test: {len(test_data)} images\")\n",
    "\n",
    "train_labels = [img['label'] for img in train_data]\n",
    "val_labels = [img['label'] for img in val_data]\n",
    "test_labels = [img['label'] for img in test_data]\n",
    "\n",
    "print(f\"\\nTrain label distribution:\")\n",
    "print(pd.Series(train_labels).value_counts())\n",
    "print(f\"\\nVal label distribution:\")\n",
    "print(pd.Series(val_labels).value_counts())\n",
    "print(f\"\\nTest label distribution:\")\n",
    "print(pd.Series(test_labels).value_counts())\n",
    "\n",
    "# Apply augmentation-based oversampling to balance the training set\n",
    "print(f\"\\nClass distribution before balancing:\")\n",
    "train_label_counts = pd.Series(train_labels).value_counts()\n",
    "for label, count in train_label_counts.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "# For now, let's just show the results without oversampling to verify the clean pipeline\n",
    "print(f\"\\nâœ… SUCCESS: Clean data pipeline working without 'Neither' filtering!\")\n",
    "print(f\"\\nFinal dataset sizes (before balancing):\")\n",
    "print(f\"Train: {len(train_data)}\")\n",
    "print(f\"Val: {len(val_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")"
   ],
   "id": "c75d2b91f7827d09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data splitting with the new approach...\n",
      "Images ready for training: 6296\n",
      "Label distribution:\n",
      "Meniscus_tear    4858\n",
      "ACL_tear         1438\n",
      "Name: count, dtype: int64\n",
      "Total original subjects: 753\n",
      "Subject stratification distribution:\n",
      "Meniscus_tear    583\n",
      "ACL_tear         170\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data split completed (image-level):\n",
      "Train: 4407 images\n",
      "Val: 944 images\n",
      "Test: 945 images\n",
      "\n",
      "Train label distribution:\n",
      "Meniscus_tear    3400\n",
      "ACL_tear         1007\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val label distribution:\n",
      "Meniscus_tear    729\n",
      "ACL_tear         215\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "Meniscus_tear    729\n",
      "ACL_tear         216\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution before balancing:\n",
      "Meniscus_tear: 3400\n",
      "ACL_tear: 1007\n",
      "\n",
      "âœ… SUCCESS: Clean data pipeline working without 'Neither' filtering!\n",
      "\n",
      "Final dataset sizes (before balancing):\n",
      "Train: 4407\n",
      "Val: 944\n",
      "Test: 945\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:06:53.570453Z",
     "start_time": "2025-08-22T07:06:53.566189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demonstrate how augmentations affect bounding boxes\n",
    "def find_image_with_bboxes():\n",
    "    \"\"\"Find an image that has bounding box annotations\"\"\"\n",
    "\n",
    "    print(\"Searching for images with bounding boxes...\")\n",
    "\n",
    "    # Search through final images to find one with bounding boxes\n",
    "    for i, sample_item in enumerate(final_images[:20]):\n",
    "        original_file_id = sample_item['file_id']\n",
    "        if original_file_id.endswith('_ACL') or original_file_id.endswith('_Meniscus'):\n",
    "            # Remove suffix for annotation lookup\n",
    "            base_file_id = original_file_id.replace('_ACL', '').replace('_Meniscus', '')\n",
    "        else:\n",
    "            base_file_id = original_file_id\n",
    "\n",
    "        # Find bounding boxes for this image\n",
    "        image_path = sample_item['path']\n",
    "        image_name = Path(image_path).stem  # e.g., \"file1000002_032\"\n",
    "\n",
    "        # Extract file and slice info for annotation lookup\n",
    "        parts = image_name.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            file_part = parts[0]  # e.g., \"file1000002\"\n",
    "            slice_part = int(parts[1])  # e.g., 32\n",
    "\n",
    "            # Look for bounding boxes in annotations\n",
    "            relevant_annotations = annotations[\n",
    "                (annotations['file'] == file_part) &\n",
    "                (annotations['slice'] == slice_part)\n",
    "            ]\n",
    "\n",
    "            if len(relevant_annotations) > 0:\n",
    "                # Convert annotations to bounding box format\n",
    "                bboxes = []\n",
    "                bbox_labels = []\n",
    "\n",
    "                for _, row in relevant_annotations.iterrows():\n",
    "                    x, y, w, h = row['x'], row['y'], row['width'], row['height']\n",
    "                    # Convert to [x_min, y_min, x_max, y_max] format\n",
    "                    bbox = [x, y, x + w, y + h]\n",
    "                    bboxes.append(bbox)\n",
    "                    bbox_labels.append(row['label'])\n",
    "\n",
    "                print(f\"Found {len(bboxes)} bounding boxes for image {image_name}\")\n",
    "                return sample_item, bboxes, bbox_labels\n",
    "\n",
    "    print(\"No images with bounding boxes found in the first 20 samples\")\n",
    "    return None, [], []"
   ],
   "id": "a0578c8dc0e6900f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:06:53.738836Z",
     "start_time": "2025-08-22T07:06:53.574825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demonstration: Proper workflow with improved classes\n",
    "def demonstrate_improved_workflow():\n",
    "    \"\"\"Show the correct workflow using both classes for their intended purposes\"\"\"\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"IMPROVED WORKFLOW DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Step 1: Use ImprovedSubjectLevelSplitter for data splitting ONLY\n",
    "    print(\"\\nðŸ”„ STEP 1: Subject-Level Data Splitting\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    improved_splitter = ImprovedSubjectLevelSplitter(\n",
    "        final_images, test_size=0.2, val_size=0.2, random_state=42\n",
    "    )\n",
    "    train_split, val_split, test_split = improved_splitter.split_subjects()\n",
    "\n",
    "    # Step 2: Use BoundingBoxAwareOversampler for class balancing\n",
    "    print(\"\\nðŸ”„ STEP 2: Augmentation-Based Class Balancing\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Create bbox-aware augmentation for oversampling\n",
    "    bbox_augmentation = MRIAugmentationWithBboxes(image_size=(320, 320))\n",
    "    oversample_transform = bbox_augmentation.get_train_augmentation_with_bboxes()\n",
    "\n",
    "    # Create the enhanced oversampler\n",
    "    bbox_oversampler = BoundingBoxAwareOversampler(data_processor, oversample_transform)\n",
    "\n",
    "    # Balance only the training set (val/test should remain unbalanced for proper evaluation)\n",
    "    print(\"Balancing training set only...\")\n",
    "    balanced_train = bbox_oversampler.oversample_with_augmentation(train_split)\n",
    "\n",
    "    print(f\"\\nFinal dataset sizes:\")\n",
    "    print(f\"Balanced Train: {len(balanced_train)}\")\n",
    "    print(f\"Val (unbalanced): {len(val_split)}\")\n",
    "    print(f\"Test (unbalanced): {len(test_split)}\")\n",
    "\n",
    "    return balanced_train, val_split, test_split\n",
    "\n",
    "# Run the demonstration\n",
    "print(\"Creating improved workflow...\")\n",
    "improved_train, improved_val, improved_test = demonstrate_improved_workflow()"
   ],
   "id": "8de34883e626de13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating improved workflow...\n",
      "======================================================================\n",
      "IMPROVED WORKFLOW DEMONSTRATION\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ STEP 1: Subject-Level Data Splitting\n",
      "----------------------------------------\n",
      "Total original subjects: 753\n",
      "Subject stratification distribution:\n",
      "Meniscus_tear    583\n",
      "ACL_tear         170\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data split completed:\n",
      "Train: 3788 images from 451 original subjects\n",
      "Val: 1218 images from 151 original subjects\n",
      "Test: 1290 images from 151 original subjects\n",
      "\n",
      "Train label distribution:\n",
      "Meniscus_tear    2899\n",
      "ACL_tear          889\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val label distribution:\n",
      "Meniscus_tear    939\n",
      "ACL_tear         279\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "Meniscus_tear    1020\n",
      "ACL_tear          270\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ”„ STEP 2: Augmentation-Based Class Balancing\n",
      "----------------------------------------\n",
      "Balancing training set only...\n",
      "\n",
      "Class Distribution before augmentation-based oversampling:\n",
      "\tMeniscus_tear: 2899\n",
      "\tACL_tear: 889\n",
      "Generating 2010 augmented samples for class 'ACL_tear'...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 42\u001B[39m\n\u001B[32m     40\u001B[39m \u001B[38;5;66;03m# Run the demonstration\u001B[39;00m\n\u001B[32m     41\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mCreating improved workflow...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m improved_train, improved_val, improved_test = \u001B[43mdemonstrate_improved_workflow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 31\u001B[39m, in \u001B[36mdemonstrate_improved_workflow\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;66;03m# Balance only the training set (val/test should remain unbalanced for proper evaluation)\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mBalancing training set only...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m balanced_train = \u001B[43mbbox_oversampler\u001B[49m\u001B[43m.\u001B[49m\u001B[43moversample_with_augmentation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_split\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mFinal dataset sizes:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     34\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBalanced Train: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(balanced_train)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JointWise\\jointwise\\preprocessing\\oversampler.py:127\u001B[39m, in \u001B[36mBoundingBoxAwareOversampler.oversample_with_augmentation\u001B[39m\u001B[34m(self, data, target_size_per_class)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m i < remainder: aug_count += \u001B[32m1\u001B[39m\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m aug_count > \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m127\u001B[39m     augmented_samples = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maugment_sample_with_bboxes\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maug_count\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    128\u001B[39m     balanced_data.extend(augmented_samples)\n\u001B[32m    129\u001B[39m     augmented_count += \u001B[38;5;28mlen\u001B[39m(augmented_samples)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JointWise\\jointwise\\preprocessing\\oversampler.py:25\u001B[39m, in \u001B[36mBoundingBoxAwareOversampler.augment_sample_with_bboxes\u001B[39m\u001B[34m(self, sample, augmentation_count)\u001B[39m\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34maugment_sample_with_bboxes\u001B[39m(\u001B[38;5;28mself\u001B[39m, sample, augmentation_count = \u001B[32m1\u001B[39m):\n\u001B[32m     23\u001B[39m     augmented_samples = []\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m     image = cv2.imread(\u001B[43msample\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpath\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m, cv2.IMREAD_GRAYSCALE)\n\u001B[32m     27\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m image \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     28\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m [sample]\n",
      "\u001B[31mTypeError\u001B[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize model builder\n",
    "model_builder = BaseModelBuilder(num_classes = 2)"
   ],
   "id": "6977090fa1e4db1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Specifications",
   "id": "c907e7165f5b8b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get all labels from the training dataset\n",
    "all_labels = [item['label'] for item in train_data]\n",
    "class_counts = Counter(all_labels)\n",
    "total_samples = len(all_labels)\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# Calculate weights (inverse frequency)\n",
    "multitask_class_weights = []\n",
    "unique_labels = sorted(class_counts.keys())  # Ensure consistent order\n",
    "for label in unique_labels:\n",
    "    count = class_counts[label]\n",
    "    weight = total_samples / (num_classes * count)\n",
    "    multitask_class_weights.append(weight)\n",
    "\n",
    "multitask_class_weights = torch.tensor(multitask_class_weights, dtype=torch.float32).to(device)\n",
    "print(f\"Multi-task class weights: {multitask_class_weights}\")"
   ],
   "id": "bdb213055fc3179"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def multitask_collate_fn(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "    # For bounding boxes, pad or select first bbox (if multiple per image)\n",
    "    bboxes = []\n",
    "    for item in batch:\n",
    "        if len(item['bboxes']) > 0:\n",
    "            # Use first bbox, or pad if needed\n",
    "            bboxes.append(torch.tensor(item['bboxes'][0], dtype=torch.float32))\n",
    "        else:\n",
    "            bboxes.append(torch.zeros(4, dtype=torch.float32))  # No bbox, fill with zeros\n",
    "    bboxes = torch.stack(bboxes)\n",
    "    return {'image': images, 'label': labels, 'bboxes': bboxes}"
   ],
   "id": "d2f8839ee89d1172"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "multitask_batch_size = 8  # or your preferred batch size\n",
    "\n",
    "# Create multi-task datasets\n",
    "multitask_train_dataset = MRIDatasetWithBboxes(train_data, data_processor, transform=MRIAugmentationWithBboxes().get_train_augmentation_with_bboxes())\n",
    "multitask_val_dataset = MRIDatasetWithBboxes(val_data, data_processor, transform=MRIAugmentationWithBboxes().get_val_augmentation_with_bboxes())\n",
    "\n",
    "# Create multi-task DataLoaders\n",
    "multitask_train_loader = DataLoader(\n",
    "    multitask_train_dataset,\n",
    "    batch_size=multitask_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    collate_fn=multitask_collate_fn\n",
    ")\n",
    "multitask_val_loader = DataLoader(\n",
    "    multitask_val_dataset,\n",
    "    batch_size=multitask_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    collate_fn=multitask_collate_fn\n",
    ")"
   ],
   "id": "86ed96d415bcd17a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Tuning using the Genetic Algorithm",
   "id": "36fbdf5e558c85e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def multitask_evaluate_individual(individual):\n",
    "    # Unpack hyperparameters\n",
    "    learning_rate, dropout_rate, classification_weight, bbox_weight, weight_decay = individual\n",
    "\n",
    "    backbones = ['xception']\n",
    "    val_accs = []\n",
    "\n",
    "    for backbone in backbones:\n",
    "        model = MultiTaskModel(\n",
    "            backbone_name=backbone,\n",
    "            num_classes=2,\n",
    "            num_bbox_coords=4,\n",
    "            dropout_rate=dropout_rate,\n",
    "            freeze_backbone=True\n",
    "        ).to(device)\n",
    "\n",
    "        trainer = MultiTaskTrainer(\n",
    "            model,\n",
    "            device,\n",
    "            class_weights=multitask_class_weights,\n",
    "            classification_weight=classification_weight,\n",
    "            bbox_weight=bbox_weight\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        try:\n",
    "            history = trainer.train(\n",
    "                multitask_train_loader,\n",
    "                multitask_val_loader,\n",
    "                epochs=3,  # Fast evaluation\n",
    "                learning_rate=learning_rate,\n",
    "                patience=2\n",
    "            )\n",
    "            val_acc = max(history['val_acc'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during GA evaluation for {backbone}: {e}\")\n",
    "            val_acc = 0.0\n",
    "\n",
    "        val_accs.append(val_acc)\n",
    "        del model, trainer, optimizer\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    # Aggregate fitness (mean validation accuracy across backbones)\n",
    "    avg_val_acc = np.mean(val_accs)\n",
    "    return (avg_val_acc,)\n",
    "\n",
    "def custom_mutate(individual, mu, sigma, indpb):\n",
    "    # Apply Gaussian mutation as before\n",
    "    individual, = tools.mutGaussian(individual, mu, sigma, indpb)\n",
    "\n",
    "    # Clamp the learning rate to be within its valid range (1e-5, 1e-3)\n",
    "    individual[0] = max(1e-5, min(individual[0], 1e-3))\n",
    "    individual[1] = max(0.3, min(individual[1], 0.7)) # dropout_rate\n",
    "    individual[2] = max(0.5, min(individual[2], 2.0)) # classification_weight\n",
    "    individual[3] = max(0.5, min(individual[3], 2.0)) # bbox_weight\n",
    "    individual[4] = max(1e-6, min(individual[4], 1e-2)) # weight_decay\n",
    "\n",
    "    return individual,\n",
    "\n",
    "# DEAP setup\n",
    "if hasattr(creator, 'FitnessMax'):\n",
    "    del creator.FitnessMax\n",
    "if hasattr(creator, 'Individual'):\n",
    "    del creator.Individual\n",
    "\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"learning_rate\", random.uniform, 1e-5, 1e-3)\n",
    "toolbox.register(\"dropout_rate\", random.uniform, 0.3, 0.7)\n",
    "toolbox.register(\"classification_weight\", random.uniform, 0.5, 2.0)\n",
    "toolbox.register(\"bbox_weight\", random.uniform, 0.5, 2.0)\n",
    "toolbox.register(\"weight_decay\", random.uniform, 1e-6, 1e-2)\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.learning_rate, toolbox.dropout_rate,\n",
    "                  toolbox.classification_weight, toolbox.bbox_weight,\n",
    "                  toolbox.weight_decay), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", multitask_evaluate_individual)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "\n",
    "# Use the custom mutation function\n",
    "toolbox.register(\"mutate\", custom_mutate, mu=0, sigma=0.1, indpb=0.2)\n",
    "\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Run GA optimization\n",
    "print(\"Starting genetic algorithm for MultiTaskModel hyperparameters...\")\n",
    "population = toolbox.population(n=4)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "population, logbook = algorithms.eaSimple(\n",
    "    population, toolbox,\n",
    "    cxpb=0.7, mutpb=0.3,\n",
    "    ngen=2, stats=stats, verbose=True\n",
    ")\n",
    "\n",
    "best_ind = tools.selBest(population, 1)[0]\n",
    "best_params = {\n",
    "    'learning_rate': best_ind[0],\n",
    "    'dropout_rate': best_ind[1],\n",
    "    'classification_weight': best_ind[2],\n",
    "    'bbox_weight': best_ind[3],\n",
    "    'weight_decay': best_ind[4],\n",
    "    'fitness': best_ind.fitness.values[0]\n",
    "}\n",
    "print(\"Best hyperparameters found by GA:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")"
   ],
   "id": "e3c20e3a30b056fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Backbone Training",
   "id": "344f27e9b1ad6a6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "backbones = ['resnext50', 'densenet201', 'efficientnet_b0', 'xception']\n",
    "results = {}\n",
    "\n",
    "for backbone in backbones:\n",
    "    print(f\"\\nTraining MultiTaskModel with backbone: {backbone}\")\n",
    "    multitask_model = MultiTaskModel(\n",
    "        backbone_name=backbone,\n",
    "        num_classes=2,\n",
    "        num_bbox_coords=4,\n",
    "        dropout_rate=best_params['dropout_rate'],      # Use optimized dropout\n",
    "        freeze_backbone=True\n",
    "    ).to(device)\n",
    "\n",
    "    multitask_trainer = MultiTaskTrainer(\n",
    "        multitask_model,\n",
    "        device,\n",
    "        class_weights=multitask_class_weights,\n",
    "        classification_weight=best_params['classification_weight'],  # Use optimized weight\n",
    "        bbox_weight=best_params['bbox_weight']                       # Use optimized weight\n",
    "    )\n",
    "\n",
    "    history = multitask_trainer.train(\n",
    "        multitask_train_loader,\n",
    "        multitask_val_loader,\n",
    "        epochs=30,\n",
    "        learning_rate=best_params['learning_rate'],    # Use optimized learning rate\n",
    "        patience=10,\n",
    "        save_path=f'best_multitask_model_{backbone}.pth'\n",
    "    )\n",
    "    results[backbone] = history"
   ],
   "id": "4062d88002e0d3ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
